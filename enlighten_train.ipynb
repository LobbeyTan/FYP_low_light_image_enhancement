{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxGqRkzvVQDd"
   },
   "source": [
    "### Only Run on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNlTlTyMVJze"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtiVqH69Vdyy"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/github/FYP_low_light_image_enhancement/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYeJ13s-V08m"
   },
   "source": [
    "# Low Light Image Enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAW01e-jDpXn"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o6WjKpXCDpXn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ILLEGEAR\\Desktop\\cheelam\\FYP\\Repositories\\FYP_low_light_image_enhancement\\venv\\lib\\site-packages\\torchvision\\models\\detection\\anchor_utils.py:63: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  device: torch.device = torch.device(\"cpu\"),\n"
     ]
    }
   ],
   "source": [
    "from data.custom_image_dataset import CustomImageDataset\n",
    "from models.enlighten import EnlightenGAN\n",
    "from configs.option import Option\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DWpEln6eDpXo"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDJDOPv0DpXo"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uJmjDvALDpXp"
   },
   "outputs": [],
   "source": [
    "img_dir = \"./datasets/light_enhancement\"\n",
    "checkpoint_dir = \"./checkpoints/enlightenGAN/\"\n",
    "batch_size = 32\n",
    "batch_shuffle = True\n",
    "\n",
    "lr = 0.0001\n",
    "\n",
    "n_epochs = 100\n",
    "print_freq = 1000\n",
    "save_freq = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilbSfL96DpXp"
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2RMeyJ7DpXq"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = CustomImageDataset(\n",
    "    img_dir=img_dir,\n",
    "    opt=Option(phase=\"train\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcaEzCVyDpXq"
   },
   "outputs": [],
   "source": [
    "# Load into dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=batch_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jd11bVZuDpXq"
   },
   "outputs": [],
   "source": [
    "dataloader_size = len(dataloader)\n",
    "\n",
    "print(\"The number of training images = %d\" % dataloader_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AytoHAPJDpXq"
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oW4nRMpIDpXr"
   },
   "outputs": [],
   "source": [
    "model = EnlightenGAN(use_src=True, lr=lr, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet_resize_conv(\n",
      "  (conv1_1): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (downsample_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (downsample_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (downsample_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (downsample_4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (LReLU1_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn1_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU1_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn1_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU2_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn2_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU2_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn2_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU3_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn3_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU3_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn3_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU4_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn4_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU4_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn4_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (max_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU5_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn5_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU5_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn5_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv5): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6_1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU6_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn6_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv6_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU6_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn6_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv6): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7_1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU7_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn7_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv7_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU7_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn7_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv7): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv8_1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU8_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn8_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv8_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU8_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn8_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv8): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv9_1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU9_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (bn9_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv9_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (LReLU9_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (conv10): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (tanh): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "print(models.vgg16().features[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znQUs7pIDpXr"
   },
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-tzIRs6DpXr"
   },
   "outputs": [],
   "source": [
    "total_iterations = 0\n",
    "train_start_time = time.time()\n",
    "\n",
    "n_print = 1\n",
    "n_save = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    epoch_iter = 0\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        model.set_input(data)\n",
    "        model.optimize_parameters()\n",
    "\n",
    "        total_iterations += len(data['img_A'])\n",
    "        epoch_iter += len(data['img_A'])\n",
    "\n",
    "        if total_iterations > (print_freq * n_print):\n",
    "            time_taken = time.time() - train_start_time\n",
    "\n",
    "            print(\"--------------------E%d-----------------------\" % (epoch+1))\n",
    "            print(\"Current Iteration: %05d | Epoch Iteration: %05d\" % (print_freq * n_print, epoch_iter))\n",
    "            print(\"Current Time Taken: %07ds | Current Epoch Running Time: %07ds\" % (time_taken, time.time() - start_time))\n",
    "            print(\"SPA Loss: %.7f | Color Loss: %.7f\" % (model.loss_spa, model.loss_color))\n",
    "            print(\"RAGAN Loss for Global D: %.7f | Local D: %.7f\" % (model.loss_D, model.loss_patch_D))\n",
    "            print(\"RAGAN Loss for Global G: %.7f | Local G: %.7f\" % (model.loss_G, model.loss_G_patch))\n",
    "            print(\"SFP Loss for Global G  : %.7f | Local G: %.7f\" % (model.loss_G_SFP, model.loss_G_SFP_patch))\n",
    "            print(f\"Total generator loss: {model.total_loss_G}\")\n",
    "            n_print += 1\n",
    "\n",
    "        if total_iterations > (save_freq * n_save):\n",
    "            print(\"Saving models...\")\n",
    "            model.save_model(checkpoint_dir, save_freq * n_save)\n",
    "            n_save += 1\n",
    "            \n",
    "\n",
    "print(f\"Total time taken: {time.time() - train_start_time}\")\n",
    "print(\"Saving trained model ...\")\n",
    "model.save_model(checkpoint_dir, epoch=\"trained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "739UIccYDpXr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9b618401b2c0edb6095b3a6b1bd3225b1d93920d52d2d6324020ed64f706650"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
